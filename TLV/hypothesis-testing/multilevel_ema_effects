import pandas as pd
import numpy as np
from pathlib import Path
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy import stats
import logging
from datetime import datetime

class FragmentationAnalysis:
    def __init__(self, input_path: str, output_dir: str):
        self.input_path = Path(input_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self._setup_logging()
        self.results = []
        
    def _setup_logging(self):
        log_path = self.output_dir / 'analysis.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_path),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def load_and_preprocess_data(self):
        self.logger.info("Loading data from %s", self.input_path)
        df = pd.read_csv(self.input_path)
        
        # Log missing values
        missing_summary = df.isnull().sum()[df.isnull().sum() > 0]
        if not missing_summary.empty:
            self.logger.info("\nMissing values summary:")
            self.logger.info(missing_summary)
        
        # Define key variables
        key_vars = [
            'digital_frag_during_mobility', 'moving_fragmentation_index',
            'digital_fragmentation_index', 'total_time_on_device',
            'total_duration_mobility'
        ]
        df_clean = df.dropna(subset=key_vars)
        
        self.logger.info(f"\nFinal sample size: {len(df_clean)} observations")
        self.data = df_clean
        return df_clean

    def prepare_within_between_data(self, data, var):
        participant_means = data.groupby('participant_id')[var].transform('mean')
        within_var = data[var] - participant_means
        between_var = participant_means - participant_means.mean()
        return within_var, between_var

    def run_stepwise_analysis(self):
        dependent_vars = [
            'TENSE', 'RELAXATION_R', 'WORRY', 'PEACE_R', 
            'IRRITATION', 'SATISFACTION_R', 'STAI6_score', 'HAPPY'
        ]
        
        independent_vars = [
            'digital_frag_during_mobility',
            'moving_fragmentation_index',
            'digital_fragmentation_index'
        ]
        
        control_vars = [
            'total_time_on_device',
            'total_duration_mobility'
        ]
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for dep_var in dependent_vars:
            for indep_var in independent_vars:
                self.logger.info(f"\nAnalyzing {dep_var} ~ {indep_var}")
                
                # Start with base model (no controls)
                base_vars = [dep_var, indep_var, 'participant_id']
                model_data = self.data[base_vars].dropna()
                
                if len(model_data) == 0:
                    self.logger.error(f"Insufficient data for {dep_var} ~ {indep_var}")
                    continue
                
                # Run models with incrementally added controls
                control_combinations = []
                for i in range(len(control_vars) + 1):
                    for combo in [control_vars[:i]]:
                        control_combinations.append(combo)
                
                for step, current_controls in enumerate(control_combinations):
                    model_name = f"Step {step}"
                    if step == 0:
                        model_name = "Base Model"
                    else:
                        model_name = f"Model {step} + {', '.join(current_controls)}"
                    
                    # Prepare variables for current model
                    current_vars = [dep_var, indep_var] + current_controls + ['participant_id']
                    current_data = self.data[current_vars].dropna()
                    
                    # Compute within and between components
                    for var in [indep_var] + current_controls:
                        within, between = self.prepare_within_between_data(current_data, var)
                        current_data[f'{var}_within'] = within
                        current_data[f'{var}_between'] = between
                    
                    # Create model formula
                    within_terms = [f'{var}_within' for var in [indep_var] + current_controls]
                    between_terms = [f'{var}_between' for var in [indep_var] + current_controls]
                    formula = f"{dep_var} ~ {' + '.join(within_terms + between_terms)} + (1|participant_id)"
                    
                    try:
                        model = smf.mixedlm(formula, data=current_data, groups='participant_id')
                        results = model.fit()
                        
                        # Extract key statistics for main predictor only
                        result_dict = {
                            'dependent_var': dep_var,
                            'independent_var': indep_var,
                            'model': model_name,
                            'n_observations': len(current_data),
                            'n_participants': current_data['participant_id'].nunique(),
                            'aic': results.aic,
                            'bic': results.bic,
                            # Within-person effects
                            'within_coef': results.params[f'{indep_var}_within'],
                            'within_std_err': results.bse[f'{indep_var}_within'],
                            'within_p_value': 2 * (1 - stats.t.cdf(abs(results.params[f'{indep_var}_within'] / 
                                                                     results.bse[f'{indep_var}_within']), 
                                                                 df=len(current_data) - len(results.params))),
                            # Between-person effects
                            'between_coef': results.params[f'{indep_var}_between'],
                            'between_std_err': results.bse[f'{indep_var}_between'],
                            'between_p_value': 2 * (1 - stats.t.cdf(abs(results.params[f'{indep_var}_between'] / 
                                                                      results.bse[f'{indep_var}_between']), 
                                                                  df=len(current_data) - len(results.params))),
                            'controls': ', '.join(current_controls) if current_controls else 'None'
                        }
                        
                        self.results.append(result_dict)
                        
                    except Exception as e:
                        self.logger.error(f"Error in {model_name}: {str(e)}")
        
        # Save results in a more readable format
        if self.results:
            results_df = pd.DataFrame(self.results)
            
            # Format p-values and coefficients
            for col in ['within_coef', 'between_coef']:
                results_df[col] = results_df[col].round(3)
            for col in ['within_p_value', 'between_p_value']:
                results_df[col] = results_df[col].round(4)
            
            # Create a single consolidated sheet with sections
            consolidated_results = []
            
            for dep_var in dependent_vars:
                for indep_var in independent_vars:
                    # Add section header
                    header_row = pd.DataFrame({
                        'model': [f"\n=== {dep_var} ~ {indep_var} ===\n"],
                        'n_observations': [''],
                        'n_participants': [''],
                        'within_coef': [''],
                        'within_std_err': [''],
                        'within_p_value': [''],
                        'between_coef': [''],
                        'between_std_err': [''],
                        'between_p_value': [''],
                        'controls': [''],
                        'aic': [''],
                        'bic': ['']
                    })
                    
                    # Get subset of results for this DV-IV combination
                    subset = results_df[
                        (results_df['dependent_var'] == dep_var) & 
                        (results_df['independent_var'] == indep_var)
                    ]
                    
                    if not subset.empty:
                        # Reorder columns for readability
                        cols_order = [
                            'model', 'n_observations', 'n_participants',
                            'within_coef', 'within_std_err', 'within_p_value',
                            'between_coef', 'between_std_err', 'between_p_value',
                            'controls', 'aic', 'bic'
                        ]
                        subset = subset[cols_order]
                        
                        # Add header and subset to consolidated results
                        consolidated_results.append(header_row)
                        consolidated_results.append(subset)
                        
                        # Add blank row after each section
                        blank_row = pd.DataFrame([[''] * len(cols_order)], columns=cols_order)
                        consolidated_results.append(blank_row)
            
            # Combine all results
            final_df = pd.concat(consolidated_results, ignore_index=True)
            
            # Save to Excel with formatting
            with pd.ExcelWriter(self.output_dir / f'stepwise_results_{timestamp}.xlsx', engine='xlsxwriter') as writer:
                final_df.to_excel(writer, sheet_name='Results', index=False)
                
                # Get workbook and worksheet objects for formatting
                workbook = writer.book
                worksheet = writer.sheets['Results']
                
                # Define formats
                header_format = workbook.add_format({
                    'bold': True,
                    'font_size': 12,
                    'bg_color': '#E0E0E0'
                })
                
                # Set column widths
                worksheet.set_column('A:A', 30)  # Model column
                worksheet.set_column('B:C', 15)  # N obs and N participants
                worksheet.set_column('D:I', 12)  # Coefficients and p-values
                worksheet.set_column('J:J', 25)  # Controls
                worksheet.set_column('K:L', 10)  # AIC and BIC
                
                # Format headers
                for col_num, value in enumerate(cols_order):
                    worksheet.write(0, col_num, value, header_format)
            
            self.logger.info(f"Saved stepwise results to {self.output_dir}/stepwise_results_{timestamp}.xlsx")

def main():
    input_path = '/Users/noamgal/Downloads/Research-Projects/SURREAL/Amnon/metrics/prepared_metrics.csv'
    output_dir = '/Users/noamgal/Downloads/Research-Projects/SURREAL/Amnon/analysis_results'
    
    
    analyzer = FragmentationAnalysis(input_path, output_dir)
    analyzer.load_and_preprocess_data()
    analyzer.run_stepwise_analysis()
    
    print("Analysis completed successfully")

if __name__ == "__main__":
    main()